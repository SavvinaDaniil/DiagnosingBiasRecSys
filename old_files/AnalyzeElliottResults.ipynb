{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb75ddd-2c26-4232-a27f-252ec66c1436",
   "metadata": {},
   "source": [
    "In this notebook, we analyze the results of NGCF trained using the Elliot framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5bfc1-f05b-434b-9b19-17e82e38824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "import os\n",
    "from data_generation import generate_data\n",
    "from itertools import chain\n",
    "import modelling_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c39d36-bfe8-4a95-9eb3-c110410a2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"fairbook\"\n",
    "# user-item interactions\n",
    "fairbook_ratings = pd.read_csv(\"data/\" + data + \"_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dfbf9-1274-408c-ad1c-ecdb947f2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = \"user\"  # the name of the column that includes the users\n",
    "item_col = \"item\"  # the name of the column that includes the items\n",
    "predict_col = \"rating\"  # the name of the column that includes the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe75014-3eff-4565-8192-e5ac8ca450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_strategies = [\n",
    "    \"uniformly_random\",\n",
    "    \"popularity_good\",\n",
    "    \"popularity_bad\",\n",
    "    \"popularity_good_for_bp_ur\",\n",
    "    \"popularity_bad_for_bp_ur\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741ab1-2919-40c0-8126-93df69f8ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"Elliotresults/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85dc8ae-ee86-4085-84c8-7ca835b0e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "keep_interval = 10\n",
    "skip_interval = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a091b1c-a49a-4907-b6bc-b285a3c9cc61",
   "metadata": {},
   "source": [
    "# Uniformly random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d106aa7-4fac-4006-972f-3b26058af6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_location = \"UniRand-\"\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5a801-939d-4c39-a5cc-b8ac32028245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = generate_data(\n",
    "    strategy=data_strategies[j], copying_dataset=fairbook_ratings, user_perc=0.2\n",
    ")\n",
    "all_items = set(ratings.item.unique())\n",
    "# sample = xf.SampleFrac(0.2, rng_spec=0)\n",
    "# sets = [i for i in enumerate(xf.partition_users(ratings,5, sample,rng_spec=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb676877-425c-4351-bc26-d311d30bd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ur_metrics = []\n",
    "for i in range(5):\n",
    "    current_location = location + added_location + str(i + 1) + \"/\"\n",
    "    print(current_location)\n",
    "    result_file = current_location + os.listdir(current_location)[0]\n",
    "    print(result_file)\n",
    "    # sett = sets[i]\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_train.csv\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_test.csv\"\n",
    "    )\n",
    "\n",
    "    # train_df = sett[1][0]\n",
    "    # test_df = sett[1][1]\n",
    "    test_users = test_df.user.unique()\n",
    "    result_df = pd.read_csv(result_file, sep=\"\\t\", header=None)\n",
    "    result_df.columns = [\"user\", \"item\", \"rating\"]\n",
    "    test_result = result_df[result_df.user.isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    end = len(test_result)\n",
    "    # Create an iterable of indices to keep\n",
    "    indices_to_keep = list(\n",
    "        chain.from_iterable(\n",
    "            range(t, t + keep_interval)\n",
    "            for t in range(start, end, keep_interval + skip_interval)\n",
    "        )\n",
    "    )\n",
    "    filtered_test_result = test_result.iloc[indices_to_keep].reset_index(\n",
    "        drop=True\n",
    "    )  # keep top 10\n",
    "    # filtered_test_result = test_result.copy()\n",
    "    recs_grouped = filtered_test_result.groupby([user_col])[item_col].apply(list)\n",
    "    pop_bias = modelling_mf.calculate_pop_bias_per_item(\n",
    "        all_items, item_col, user_col, predict_col, train_df, recs=filtered_test_result\n",
    "    )\n",
    "    GAP_vs_GAP = modelling_mf.calculate_ave_pop_per_user(\n",
    "        test_users, item_col, user_col, pop_bias, train_df, recs_grouped\n",
    "    )\n",
    "    pop_corr = modelling_mf.calculate_pop_correlation(pop_bias)\n",
    "    precision, recall, ndcg = modelling_mf.calculate_topn_metrics(\n",
    "        filtered_test_result, test_df\n",
    "    )\n",
    "    AggDiv = modelling_mf.evaluate_item_coverage(pop_bias[\"recommendation\"].values)\n",
    "    ARP, ave_PL, ACLT = modelling_mf.calculate_all_pb_metrics(\n",
    "        pop_bias,\n",
    "        test_users,\n",
    "        item_col,\n",
    "        user_col,\n",
    "        train_df,\n",
    "        recs_grouped,\n",
    "        filtered_test_result,\n",
    "    )\n",
    "    metrics_dict_ur = {\n",
    "        \"pop_corr\": pop_corr,\n",
    "        \"RMSE\": 0,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"ARP\": ARP,\n",
    "        \"ave_PL\": ave_PL,\n",
    "        \"ACLT\": ACLT,\n",
    "        \"AggDiv\": AggDiv,\n",
    "    }\n",
    "    ur_metrics.append(metrics_dict_ur)\n",
    "    pop_biases = [pop_bias]\n",
    "    modelling_mf.plot_results(\n",
    "        pop_biases.copy(),\n",
    "        GAP_vs_GAP.copy(),\n",
    "        \"CornacNGCF\",\n",
    "        0,\n",
    "        precision,\n",
    "        recall,\n",
    "        ndcg,\n",
    "        0,\n",
    "        0,\n",
    "        cv=False,\n",
    "        n=10,\n",
    "        args=\"fold\" + str(i + 1),\n",
    "        data_strategy=data_strategies[j],\n",
    "        save_plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca5f45-3961-4163-964b-2ea693a387c2",
   "metadata": {},
   "source": [
    "# Popularity Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952629c0-869e-45a9-bd3b-9f50bd8e584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_location = \"GoodPop-\"\n",
    "j = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975742e-15af-445b-9d56-762616a613dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings = generate_data(strategy = data_strategies[j],\n",
    "#                             copying_dataset = fairbook_ratings,\n",
    "#                             user_perc = 0.2)\n",
    "# all_items=set(ratings.item.unique())\n",
    "# sample = xf.SampleFrac(0.2, rng_spec=0)\n",
    "# sets = [i for i in enumerate(xf.partition_users(ratings,5, sample,rng_spec=0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf9326-c16b-4910-8ddd-ea16ccf0c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_metrics = []\n",
    "for i in range(5):\n",
    "    current_location = location + added_location + str(i + 1) + \"/\"\n",
    "    print(current_location)\n",
    "    if os.listdir(current_location)[0] == \".ipynb_checkpoints\":\n",
    "        result_file = current_location + os.listdir(current_location)[1]\n",
    "    else:\n",
    "        result_file = current_location + os.listdir(current_location)[0]\n",
    "    print(result_file)\n",
    "    # sett = sets[i]\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_train.csv\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_test.csv\"\n",
    "    )\n",
    "\n",
    "    # train_df = sett[1][0]\n",
    "    # test_df = sett[1][1]\n",
    "    test_users = test_df.user.unique()\n",
    "    result_df = pd.read_csv(result_file, sep=\"\\t\", header=None)\n",
    "    result_df.columns = [\"user\", \"item\", \"rating\"]\n",
    "    test_result = result_df[result_df.user.isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    end = len(test_result)\n",
    "    # Create an iterable of indices to keep\n",
    "    indices_to_keep = list(\n",
    "        chain.from_iterable(\n",
    "            range(t, t + keep_interval)\n",
    "            for t in range(start, end, keep_interval + skip_interval)\n",
    "        )\n",
    "    )\n",
    "    filtered_test_result = test_result.iloc[indices_to_keep].reset_index(\n",
    "        drop=True\n",
    "    )  # keep top 10\n",
    "    # filtered_test_result = test_result.copy()\n",
    "    recs_grouped = filtered_test_result.groupby([user_col])[item_col].apply(list)\n",
    "    pop_bias = modelling_mf.calculate_pop_bias_per_item(\n",
    "        all_items, item_col, user_col, predict_col, train_df, recs=filtered_test_result\n",
    "    )\n",
    "    GAP_vs_GAP = modelling_mf.calculate_ave_pop_per_user(\n",
    "        test_users, item_col, user_col, pop_bias, train_df, recs_grouped\n",
    "    )\n",
    "    pop_corr = modelling_mf.calculate_pop_correlation(pop_bias)\n",
    "    precision, recall, ndcg = modelling_mf.calculate_topn_metrics(\n",
    "        filtered_test_result, test_df\n",
    "    )\n",
    "    AggDiv = modelling_mf.evaluate_item_coverage(pop_bias[\"recommendation\"].values)\n",
    "    ARP, ave_PL, ACLT = modelling_mf.calculate_all_pb_metrics(\n",
    "        pop_bias,\n",
    "        test_users,\n",
    "        item_col,\n",
    "        user_col,\n",
    "        train_df,\n",
    "        recs_grouped,\n",
    "        filtered_test_result,\n",
    "    )\n",
    "    metrics_dict_pg = {\n",
    "        \"pop_corr\": pop_corr,\n",
    "        \"RMSE\": 0,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"ARP\": ARP,\n",
    "        \"ave_PL\": ave_PL,\n",
    "        \"ACLT\": ACLT,\n",
    "        \"AggDiv\": AggDiv,\n",
    "    }\n",
    "    pg_metrics.append(metrics_dict_pg)\n",
    "    pop_biases = [pop_bias]\n",
    "    modelling_mf.plot_results(\n",
    "        pop_biases.copy(),\n",
    "        GAP_vs_GAP.copy(),\n",
    "        \"CornacNGCF\",\n",
    "        0,\n",
    "        precision,\n",
    "        recall,\n",
    "        ndcg,\n",
    "        0,\n",
    "        0,\n",
    "        cv=False,\n",
    "        n=10,\n",
    "        args=\"fold\" + str(i + 1),\n",
    "        data_strategy=data_strategies[j],\n",
    "        save_plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa92995-2f77-4123-8b26-1c3b5904b7a2",
   "metadata": {},
   "source": [
    "# Popularity Bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4e265-ae69-432e-98cc-89fae0219f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_location = \"badPop-\"\n",
    "j = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20761e38-2b9c-4566-9a37-68bd88324dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_metrics = []\n",
    "for i in range(5):\n",
    "    current_location = location + added_location + str(i + 1) + \"/\"\n",
    "    print(current_location)\n",
    "    if os.listdir(current_location)[0] == \".DS_Store\":\n",
    "        result_file = current_location + os.listdir(current_location)[1]\n",
    "    else:\n",
    "        result_file = current_location + os.listdir(current_location)[0]\n",
    "    print(result_file)\n",
    "    # sett = sets[i]\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_train.csv\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_test.csv\"\n",
    "    )\n",
    "\n",
    "    # train_df = sett[1][0]\n",
    "    # test_df = sett[1][1]\n",
    "    test_users = test_df.user.unique()\n",
    "    result_df = pd.read_csv(result_file, sep=\"\\t\", header=None)\n",
    "    result_df.columns = [\"user\", \"item\", \"rating\"]\n",
    "    test_result = result_df[result_df.user.isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    end = len(test_result)\n",
    "    # Create an iterable of indices to keep\n",
    "    indices_to_keep = list(\n",
    "        chain.from_iterable(\n",
    "            range(t, t + keep_interval)\n",
    "            for t in range(start, end, keep_interval + skip_interval)\n",
    "        )\n",
    "    )\n",
    "    filtered_test_result = test_result.iloc[indices_to_keep].reset_index(\n",
    "        drop=True\n",
    "    )  # keep top 10\n",
    "    # filtered_test_result = test_result.copy()\n",
    "    recs_grouped = filtered_test_result.groupby([user_col])[item_col].apply(list)\n",
    "    pop_bias = modelling_mf.calculate_pop_bias_per_item(\n",
    "        all_items, item_col, user_col, predict_col, train_df, recs=filtered_test_result\n",
    "    )\n",
    "    GAP_vs_GAP = modelling_mf.calculate_ave_pop_per_user(\n",
    "        test_users, item_col, user_col, pop_bias, train_df, recs_grouped\n",
    "    )\n",
    "    pop_corr = modelling_mf.calculate_pop_correlation(pop_bias)\n",
    "    precision, recall, ndcg = modelling_mf.calculate_topn_metrics(\n",
    "        filtered_test_result, test_df\n",
    "    )\n",
    "    AggDiv = modelling_mf.evaluate_item_coverage(pop_bias[\"recommendation\"].values)\n",
    "    ARP, ave_PL, ACLT = modelling_mf.calculate_all_pb_metrics(\n",
    "        pop_bias,\n",
    "        test_users,\n",
    "        item_col,\n",
    "        user_col,\n",
    "        train_df,\n",
    "        recs_grouped,\n",
    "        filtered_test_result,\n",
    "    )\n",
    "    metrics_dict_pb = {\n",
    "        \"pop_corr\": pop_corr,\n",
    "        \"RMSE\": 0,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"ARP\": ARP,\n",
    "        \"ave_PL\": ave_PL,\n",
    "        \"ACLT\": ACLT,\n",
    "        \"AggDiv\": AggDiv,\n",
    "    }\n",
    "    pb_metrics.append(metrics_dict_pb)\n",
    "    pop_biases = [pop_bias]\n",
    "    modelling_mf.plot_results(\n",
    "        pop_biases.copy(),\n",
    "        GAP_vs_GAP.copy(),\n",
    "        \"CornacNGCF\",\n",
    "        0,\n",
    "        precision,\n",
    "        recall,\n",
    "        ndcg,\n",
    "        0,\n",
    "        0,\n",
    "        cv=False,\n",
    "        n=10,\n",
    "        args=\"fold\" + str(i + 1),\n",
    "        data_strategy=data_strategies[j],\n",
    "        save_plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2590af-c87b-4a42-b221-c65761fec632",
   "metadata": {},
   "source": [
    "# Popularity Good for Bp Ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62caa458-30f8-4a27-a16b-d27d5f8b064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_location = \"bp_ur-GoodPop-\"\n",
    "j = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34bcdc-68c7-415a-856d-812b423f272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgbp_metrics = []\n",
    "for i in range(5):\n",
    "    current_location = location + added_location + str(i + 1) + \"/\"\n",
    "    print(current_location)\n",
    "    result_file = current_location + os.listdir(current_location)[0]\n",
    "    print(result_file)\n",
    "    # sett = sets[i]\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_train.csv\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_test.csv\"\n",
    "    )\n",
    "\n",
    "    # train_df = sett[1][0]\n",
    "    # test_df = sett[1][1]\n",
    "    test_users = test_df.user.unique()\n",
    "    result_df = pd.read_csv(result_file, sep=\"\\t\", header=None)\n",
    "    result_df.columns = [\"user\", \"item\", \"rating\"]\n",
    "    test_result = result_df[result_df.user.isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    end = len(test_result)\n",
    "    # Create an iterable of indices to keep\n",
    "    indices_to_keep = list(\n",
    "        chain.from_iterable(\n",
    "            range(t, t + keep_interval)\n",
    "            for t in range(start, end, keep_interval + skip_interval)\n",
    "        )\n",
    "    )\n",
    "    filtered_test_result = test_result.iloc[indices_to_keep].reset_index(\n",
    "        drop=True\n",
    "    )  # keep top 10\n",
    "    # filtered_test_result = test_result.copy()\n",
    "    recs_grouped = filtered_test_result.groupby([user_col])[item_col].apply(list)\n",
    "    pop_bias = modelling_mf.calculate_pop_bias_per_item(\n",
    "        all_items, item_col, user_col, predict_col, train_df, recs=filtered_test_result\n",
    "    )\n",
    "    GAP_vs_GAP = modelling_mf.calculate_ave_pop_per_user(\n",
    "        test_users, item_col, user_col, pop_bias, train_df, recs_grouped\n",
    "    )\n",
    "    pop_corr = modelling_mf.calculate_pop_correlation(pop_bias)\n",
    "    precision, recall, ndcg = modelling_mf.calculate_topn_metrics(\n",
    "        filtered_test_result, test_df\n",
    "    )\n",
    "    AggDiv = modelling_mf.evaluate_item_coverage(pop_bias[\"recommendation\"].values)\n",
    "    ARP, ave_PL, ACLT = modelling_mf.calculate_all_pb_metrics(\n",
    "        pop_bias,\n",
    "        test_users,\n",
    "        item_col,\n",
    "        user_col,\n",
    "        train_df,\n",
    "        recs_grouped,\n",
    "        filtered_test_result,\n",
    "    )\n",
    "    metrics_dict_pgbp = {\n",
    "        \"pop_corr\": pop_corr,\n",
    "        \"RMSE\": 0,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"ARP\": ARP,\n",
    "        \"ave_PL\": ave_PL,\n",
    "        \"ACLT\": ACLT,\n",
    "        \"AggDiv\": AggDiv,\n",
    "    }\n",
    "    pgbp_metrics.append(metrics_dict_pgbp)\n",
    "    pop_biases = [pop_bias]\n",
    "    modelling_mf.plot_results(\n",
    "        pop_biases.copy(),\n",
    "        GAP_vs_GAP.copy(),\n",
    "        \"CornacNGCF\",\n",
    "        0,\n",
    "        precision,\n",
    "        recall,\n",
    "        ndcg,\n",
    "        0,\n",
    "        0,\n",
    "        cv=False,\n",
    "        n=10,\n",
    "        args=\"fold\" + str(i + 1),\n",
    "        data_strategy=data_strategies[j],\n",
    "        save_plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33378e6a-2f37-47ee-8efa-0887ee62ac01",
   "metadata": {},
   "source": [
    "# Popularity Bad for Bp Ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fc6a6-2d81-4d43-bff3-d0c626eed048",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_location = \"bp_ur-BadPop-\"\n",
    "j = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412e644-17b4-46e5-a785-65ef2d47697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbbp_metrics = []\n",
    "for i in range(5):\n",
    "    current_location = location + added_location + str(i + 1) + \"/\"\n",
    "    print(current_location)\n",
    "    if os.listdir(current_location)[0] == \".DS_Store\":\n",
    "        result_file = current_location + os.listdir(current_location)[1]\n",
    "    else:\n",
    "        result_file = current_location + os.listdir(current_location)[0]\n",
    "    print(result_file)\n",
    "    # sett = sets[i]\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_train.csv\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        \"all_folds/\" + data_strategies[j] + \"_fold_\" + str(i + 1) + \"_test.csv\"\n",
    "    )\n",
    "\n",
    "    # train_df = sett[1][0]\n",
    "    # test_df = sett[1][1]\n",
    "    test_users = test_df.user.unique()\n",
    "    result_df = pd.read_csv(result_file, sep=\"\\t\", header=None)\n",
    "    result_df.columns = [\"user\", \"item\", \"rating\"]\n",
    "    test_result = result_df[result_df.user.isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "    end = len(test_result)\n",
    "    # Create an iterable of indices to keep\n",
    "    indices_to_keep = list(\n",
    "        chain.from_iterable(\n",
    "            range(t, t + keep_interval)\n",
    "            for t in range(start, end, keep_interval + skip_interval)\n",
    "        )\n",
    "    )\n",
    "    filtered_test_result = test_result.iloc[indices_to_keep].reset_index(\n",
    "        drop=True\n",
    "    )  # keep top 10\n",
    "    # filtered_test_result = test_result.copy()\n",
    "    recs_grouped = filtered_test_result.groupby([user_col])[item_col].apply(list)\n",
    "    pop_bias = modelling_mf.calculate_pop_bias_per_item(\n",
    "        all_items, item_col, user_col, predict_col, train_df, recs=filtered_test_result\n",
    "    )\n",
    "    GAP_vs_GAP = modelling_mf.calculate_ave_pop_per_user(\n",
    "        test_users, item_col, user_col, pop_bias, train_df, recs_grouped\n",
    "    )\n",
    "    pop_corr = modelling_mf.calculate_pop_correlation(pop_bias)\n",
    "    precision, recall, ndcg = modelling_mf.calculate_topn_metrics(\n",
    "        filtered_test_result, test_df\n",
    "    )\n",
    "    AggDiv = modelling_mf.evaluate_item_coverage(pop_bias[\"recommendation\"].values)\n",
    "    ARP, ave_PL, ACLT = modelling_mf.calculate_all_pb_metrics(\n",
    "        pop_bias,\n",
    "        test_users,\n",
    "        item_col,\n",
    "        user_col,\n",
    "        train_df,\n",
    "        recs_grouped,\n",
    "        filtered_test_result,\n",
    "    )\n",
    "    metrics_dictpbbp = {\n",
    "        \"pop_corr\": pop_corr,\n",
    "        \"RMSE\": 0,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"ARP\": ARP,\n",
    "        \"ave_PL\": ave_PL,\n",
    "        \"ACLT\": ACLT,\n",
    "        \"AggDiv\": AggDiv,\n",
    "    }\n",
    "\n",
    "    pbbp_metrics.append(metrics_dictpbbp)\n",
    "    pop_biases = [pop_bias]\n",
    "    modelling_mf.plot_results(\n",
    "        pop_biases.copy(),\n",
    "        GAP_vs_GAP.copy(),\n",
    "        \"CornacNGCF\",\n",
    "        0,\n",
    "        precision,\n",
    "        recall,\n",
    "        ndcg,\n",
    "        0,\n",
    "        0,\n",
    "        cv=False,\n",
    "        n=10,\n",
    "        args=\"fold\" + str(i + 1),\n",
    "        data_strategy=data_strategies[j],\n",
    "        save_plot=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269658e-164a-44fe-a918-4040aa418a14",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc140aff-78f9-4b04-9a0e-dfd5c9251a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [ur_metrics, pg_metrics, pb_metrics, pgbp_metrics, pbbp_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed4425-ae5d-4cfb-8eb2-e3153232ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_values = [\"pop_corr\", \"ARP\", \"ave_PL\", \"AggDiv\", \"NDCG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04522217-6ca7-4d7e-9325-3a5de1eccd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_list = []\n",
    "for strategy in metrics_list:\n",
    "    curr_dict = {}\n",
    "    for value in relevant_values:\n",
    "        ave_value = sum(d[value] for d in strategy) / len(strategy)\n",
    "        curr_dict[value] = ave_value\n",
    "    strategy_list.append(curr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7e772-b2f0-4772-b114-8666c5dbce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [\"Scenario 1\", \"Scenario 2\", \"Scenario 3\", \"Scenario 4\", \"Scenario 5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcf641-e06a-4c8d-9b8a-053affd5c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(strategy_list, index=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8abb2-5d52-462f-a79f-222d226f12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4240c-ace9-4afe-946d-093386ef8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"metrics_combined/all_ngcf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result.round(3), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
