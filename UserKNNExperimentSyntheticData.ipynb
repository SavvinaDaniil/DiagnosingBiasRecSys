{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc80e674",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Choose data scenario.\n",
    "2. Generate the dataset accordingly.\n",
    "5. Choose 'fixed' configuration.\n",
    "6. For each 'fixed' configuration, optimize the other parameters based on RMSE.\n",
    "7. Given optimal setting, run popularity bias analysis for every version of the 'fixed' configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d29498",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a87c3-d3f7-4a67-ac93-081b451542c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MKL_THREADING_LAYER=tbb\n",
    "%env OPENBLAS_NUM_THREADS=24\n",
    "%env NUMBA_NUM_THREADS=96\n",
    "%env MKL_NUM_THREADS=96\n",
    "%env OMP_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f8b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"tbb\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '24'\n",
    "os.environ[\"NUMBA_NUM_THREADS\"] = '96'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '96'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "# for random generation\n",
    "import numpy as np \n",
    "import random as rd\n",
    "\n",
    "\n",
    "# basic functions\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "import pickle\n",
    "\n",
    "# custom-made functions\n",
    "import modelling_mf\n",
    "from data_generation import generate_data\n",
    "from optimize_hp import optimize_lkpy, optimize_cornac\n",
    "\n",
    "# lenskit RS library\n",
    "from lenskit.algorithms import user_knn\n",
    "\n",
    "\n",
    "# cornac RS library\n",
    "from cornac.models import UserKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cada525",
   "metadata": {},
   "source": [
    "## Fairbook data and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93470ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"fairbook\"\n",
    "# user-item interactions\n",
    "fairbook_ratings = pd.read_csv(\"data/\"+data+\"_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940275c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = \"user\" # the name of the column that includes the users\n",
    "item_col = \"item\" # the name of the column that includes the items\n",
    "predict_col=\"rating\" # the name of the column that includes the interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c854d",
   "metadata": {},
   "source": [
    "## Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa1baa",
   "metadata": {},
   "source": [
    "1. There is no general relation between popularity and rating: uniformly_random\n",
    "2. Popular items are rated higher by the users.: popularity_good\n",
    "3. Popular items are rated lower by the users.: popilarity_bad\n",
    "4. Popular items are rated higher by users with big profiles.: popularity_good_for_bp_ur\n",
    "5. Popular items are rated lower by users with big profiles.: popularity_bad_for_bp_ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fad48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_strategies = ['uniformly_random',\n",
    "                   'popularity_good',\n",
    "                   'popularity_bad',\n",
    "                   'popularity_good_for_bp_ur',\n",
    "                   'popularity_bad_for_bp_ur']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535cae7",
   "metadata": {},
   "source": [
    "## Optimize, train, evaluate LKPY\n",
    "- **Algorithm**\n",
    "- **Fixed parameters**\n",
    "- **To-optimize parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_versions = {\"UserKNN\":[{'min_nbrs':1, 'min_sim':0},\n",
    "                            {'min_nbrs':2, 'min_sim':0},\n",
    "                            {'min_nbrs':10, 'min_sim':0},\n",
    "                            {'min_nbrs':1, 'min_sim':-1},\n",
    "                            {'min_nbrs':2, 'min_sim':-1},\n",
    "                            {'min_nbrs':10, 'min_sim':-1}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_way = \"cross_validation\"\n",
    "verbose = False\n",
    "plot = True\n",
    "save_plot = True # save the plots\n",
    "fallback = False\n",
    "nr_recs = 10\n",
    "sampling_strategy = \"frac\"\n",
    "partition_way = \"user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose algorithm\n",
    "algorithm_lkpy = user_knn.UserUser\n",
    "algo_name = \"UserKNN\"\n",
    "versions = algo_versions[algo_name]\n",
    "\n",
    "# for every data strategy\n",
    "for i in range(len(data_strategies)):\n",
    "    data_strategy = data_strategies[i]\n",
    "    # generate the data \n",
    "    ratings = generate_data(strategy = data_strategy,\n",
    "                            copying_dataset = fairbook_ratings,\n",
    "                            user_perc = 0.2)\n",
    "    \n",
    "    # for every 'fixed' version of the algorithm\n",
    "    for args in versions:\n",
    "        print(data_strategy, args)\n",
    "        \n",
    "        # optimize for this fixed version\n",
    "        best_params = optimize_lkpy(ratings=ratings, algorithm_name=algo_name, args=args, partition_way='row', max_evals=20)\n",
    "\n",
    "        # save the best parameters for this fixed version\n",
    "\n",
    "        with open('best_parameters/'+algo_name+'/'+data_strategy+'_'+str(args)+'.pkl', 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "\n",
    "\n",
    "        optimal_nnbrs = best_params[\"nnbrs\"]\n",
    "\n",
    "        \n",
    "\n",
    "        # run the training and evaluation for the fixed version + the best other parameters\n",
    "        pop_biases_lkpy, metrics_dict_lkpy = modelling_mf.train_algorithm(algorithm = algorithm_lkpy(nnbrs=optimal_nnbrs,\n",
    "                                                                \n",
    "                                                                center=True,\n",
    "                                                            min_sim=args['min_sim'],\n",
    "                                                            min_nbrs=args['min_nbrs']),\n",
    "                                                        algo_name = algo_name,  \n",
    "                                                        ratings = ratings,\n",
    "                                                        evaluation_way = evaluation_way,\n",
    "                                                        verbose = verbose, \n",
    "                                                        n=nr_recs,\n",
    "                                                        sampling_strategy = sampling_strategy,\n",
    "                                                        partition_way = partition_way,\n",
    "                                                        plot = plot,\n",
    "                                                    data_strategy=data_strategy,\n",
    "                                                    args=args,\n",
    "                                                    save_plot=save_plot)\n",
    "\n",
    "        # Save metrics!\n",
    "        with open('experimental_results/'+algo_name+'/'+data_strategy+'_'+str(args)+'.pkl', 'wb') as f:\n",
    "            pickle.dump(metrics_dict_lkpy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728de63-aa2c-44a8-961d-8ddee25304f4",
   "metadata": {},
   "source": [
    "## Optimize, train, evaluate Cornac\n",
    "- **Algorithm**\n",
    "- **Fixed parameters**\n",
    "- **To-optimize parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58b0cf-fa29-4f0f-ae9e-c9d5d886d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_versions = {\"CornacUserKNN\":[{'center':True}],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_cornac = UserKNN\n",
    "algo_name = \"CornacUserKNN\"\n",
    "versions = algo_versions[algo_name]\n",
    "\n",
    "# for every data strategy\n",
    "for i in range(len(data_strategies)):\n",
    "    data_strategy = data_strategies[i]\n",
    "    # generate the data \n",
    "    ratings = generate_data(strategy = data_strategy,\n",
    "                            copying_dataset = fairbook_ratings,\n",
    "                            user_perc = 0.2)\n",
    "    # for every 'fixed' version of the algorithm\n",
    "    for args in versions:\n",
    "        print(data_strategy, args)\n",
    "        \n",
    "        # optimize for this fixed version\n",
    "        best_params = optimize_cornac(ratings=ratings, algorithm_name=algo_name, args=args, max_evals=20)\n",
    "\n",
    "        # save the best parameters for this fixed version\n",
    "\n",
    "        with open('best_parameters/'+algo_name+'/'+data_strategy+'_'+str(args)+'.pkl', 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "\n",
    "\n",
    "        optimal_k = best_params['k']\n",
    "\n",
    "        \n",
    "\n",
    "        pop_biases_cornac, metrics_dict_cornac = modelling_mf.train_algorithm_cornac(algorithm = algorithm_cornac(k=optimal_k,\n",
    "                                                                                                                 mean_centered=args['center']),\n",
    "                                                        algo_name = algo_name,  \n",
    "                                                        ratings = ratings,\n",
    "                                                        evaluation_way = evaluation_way,\n",
    "                                                        verbose = verbose, \n",
    "                                                        n=nr_recs,\n",
    "                                                        sampling_strategy = sampling_strategy,\n",
    "                                                        partition_way = partition_way,\n",
    "                                                        plot = plot,\n",
    "                                                    data_strategy=data_strategy,\n",
    "                                                    args=args,\n",
    "                                                    save_plot=save_plot)\n",
    "\n",
    "        # Save metrics!\n",
    "        with open('experimental_results/'+algo_name+'/'+data_strategy+'_'+str(args)+'.pkl', 'wb') as f:\n",
    "            pickle.dump(metrics_dict_cornac, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c3278b04384a1f2d1b0bc3e8783905713abb1db8d84b3b69ddff13fab40f022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
